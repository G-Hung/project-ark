# reference: https://github.com/tigger0jk/ark-invest-scraper/blob/main/.github/workflows/scrape.yml
# but this repo only takes the latest, while we want the snapshots in different partitions

name: Scrape snapshot data

on:
  push:
  workflow_dispatch:
  schedule:
    # just in case, I didn't study their update frequency, so let's run it 3 times a day
    # to be precise, it will run at: 0030, 0830, 1630
    - cron:  '30 0,8,16 * * *'

env:
  DBT_PROFILES_DIR: ./
  DBT_GOOGLE_PROJECT: ark-track
  DBT_GOOGLE_BIGQUERY_DATASET: raw
  # DBT_GOOGLE_BIGQUERY_KEYFILE: ./.gcloud/dbt-service-account.json

jobs:
  scheduled:
    runs-on: ubuntu-latest
    steps:
    - name: Check out this repo
      uses: actions/checkout@v2

    - name: Set up Cloud SDK
      uses: google-github-actions/setup-gcloud@master
      with:
        project_id: ${{ secrets.GCP_PROJECT_ID }}
        service_account_key: ${{ secrets.GCP_SA_KEY }}
        export_default_credentials: true
  
    - name: Fetch the latest data and push to Bigquery
      run: |-
        python -m pip install --upgrade pip --quiet
        if [ -f requirements.txt ]; then pip install -r requirements.txt --quiet; fi
        ./script.sh

    - name: Run dbt test
      run: |-
        dbt --version
        dbt debug
        dbt test

    - name: Commit and push if it changed
      run: |-
        git config user.name "Automated"
        git config user.email "actions@users.noreply.github.com"
        git add data
        timestamp=$(date -u)
        git commit -m "Latest update: ${timestamp}" || exit 0
        git push
